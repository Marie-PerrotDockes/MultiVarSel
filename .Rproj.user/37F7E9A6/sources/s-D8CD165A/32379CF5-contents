## Nouvelle Stability Selection  : 
# setwd('/run/user/1002/gvfs/smb-share:server=bianca,share=mmip/perrotdockes/enviromics')
source('/mnt/mmip/perrotdockes/illustration_numerique/getSigma12.R')
load('copals_data.RData')
source('whitening_data.R')      ### to estimate the matrix Sigma^(-1/2) according to a dependence structure (AR1 or Nonparametric)

library(MultiVarSel,lib.loc='/mnt/mmip/perrotdockes/')
load('copals_data.RData')
X <- model.matrix(~ X2 + 0)
library(parallel)
library(reshape2)
library(glmnet)
library(Matrix)

whitening<-function (residuals, typeDep, pAR = 1, qMA = 0) 
{
  n = dim(residuals)[1]
  q = dim(residuals)[2]
  if (typeDep == "no_whitening") {
    return(Diagonal(q, 1))
  }
  if (typeDep == "AR1") {
    phi_hat = c()
    for (i in 1:n) {
      phi_hat[i] = try( arima(residuals[i, ], order = c(1, 0, 
                                                   0))$coef[1],silent=T)
    }
    phi_hat_final = mean(phi_hat,na.rm=T)
    phi_hat_vect = rep(-phi_hat_final, (q - 1))
    square_root_inv_hat_Sigma = bandSparse(q, k = c(1, 0), 
                                           diagonals = list(phi_hat_vect, c(sqrt(1 - phi_hat_final^2), 
                                                                            rep(1, (q - 1)))))
    return(square_root_inv_hat_Sigma)
  }
  if (typeDep == "ARMA") {
    phi_hat = matrix(0, n, max(pAR, 1))
    theta_hat = matrix(0, n, max(qMA, 1))
    if ((pAR >= 1) && (qMA >= 1)) {
      for (i in 1:n) {
        phi_hat[i, ] = arima(residuals[i, ], order = c(pAR, 
                                                       0, qMA))$coef[1:pAR]
        theta_hat[i, ] = arima(residuals[i, ], order = c(pAR, 
                                                         0, qMA))$coef[(pAR + 1):(pAR + qMA)]
      }
    }
    if ((pAR >= 1) && (qMA == 0)) {
      for (i in 1:n) {
        phi_hat[i, ] = arima(residuals[i, ], order = c(pAR, 
                                                       0, qMA))$coef[1:pAR]
      }
    }
    if ((pAR == 0) && (qMA >= 1)) {
      for (i in 1:n) {
        theta_hat[i, ] = arima(residuals[i, ], order = c(pAR, 
                                                         0, qMA))$coef[(pAR + 1):(pAR + qMA)]
      }
    }
    phi_hat_final = colMeans(phi_hat)
    theta_hat_final = colMeans(theta_hat)
    acf_theo_hat = ARMAacf(ar = phi_hat_final, ma = theta_hat_final, 
                           lag.max = (q - 1))
    psi_hat = ARMAtoMA(ar = phi_hat_final, ma = theta_hat_final, 
                       1000)
    variance_hat = 1 + sum(psi_hat^2)
    Sigma_hat = toeplitz(acf_theo_hat) * variance_hat
    square_root_inv_hat_Sigma = Matrix(round(solve(chol(Sigma_hat)), 
                                             digits = 6))
    return(square_root_inv_hat_Sigma)
  }
  if (typeDep == "nonparam") {
    vector_cov = matrix(0, n, q)
    for (i in 1:n) {
      vector_cov[i, ] = acf(residuals[i, ], type = "covariance", 
                            plot = FALSE, lag.max = (q - 1))$acf
    }
    vector_cov_estim = colMeans(vector_cov)
    cov_matrix = toeplitz(vector_cov_estim)
    square_root_inv_hat_Sigma = Matrix(round(solve(chol(cov_matrix)), 
                                             digits = 6))
    return(square_root_inv_hat_Sigma)
  }
}
New_stab_sel_anova <-function(group,Y,nb_replis=1000,nb.cores=3,typeDep='AR1'){
  X <- model.matrix(~ group + 0)
  p <- ncol(X)
  q <- ncol(Y)
  nb_repli=max(nb_replis)
  # Première étape : CV on coupe le jeu de donnée en 10 sur chaque fold on fait une stab selection
  #on calcul bien les sample pour gardé la proportion des groupes
  sample <- rep(0,length(group))
  s<-sapply(unique(group),function(grp){
    sample[group==grp]<<-c(sample(rep(1:10,sum(group==grp)%/%10),10*(sum(group==grp)%/%10)),sample(1:10,sum(group==grp)%%10))})
  # Par fold (enfin par groupe d'individus quand on retire chaque fold) on:
  Par_fold <-function(i){
    # Calcul de Sigma et Xrond et yrond :
    residuals=lm(as.matrix(Y[sample!=i,])~X[sample!=i,]-1)$residuals
    square_root_inv_hat_Sigma <- whitening(residuals,typeDep,pAR=1,qMA=0)
    Yr = as.numeric(Y[sample!=i,] %*% square_root_inv_hat_Sigma)
    Xr = kronecker(t(square_root_inv_hat_Sigma), X[sample!=i,])
    # Calcul de Lambda min CV :
    Lmin <- cv.glmnet(Xr,Yr,intercept=F)$lambda.min
    
    # On lance ensuite une étape de stability selection pour 'secouer un peu tout ça'
    Res <- mclapply(1:nb_repli,function(lala){
      grps <- rep(group[sample!=i],q)
      # On selectionne proportionellement à la taille des groupes
      sel <- sort(unlist(sapply(unique(grps),function(grp){sample(which(grps==grp),round(sum(grps==grp)/2))})))
      resultat_glmnet = glmnet(Xr[sel, ], Yr[sel],
                               family = "gaussian", alpha = 1, lambda = Lmin,intercept=F)
      
      ind_glmnet = which(resultat_glmnet$beta != 0)
      return(tabulate(ind_glmnet, (p * q)))
    }   ,
    mc.cores = nb.cores)
    freq <- do.call(cbind, lapply(nb_replis,function(x){
      Reduce("+", Res[1:x])/x
    }))
   colnames(freq) <- nb_replis
    #on recupeère les frequences
    return(freq)
    
  }
  if (is.null(colnames(Y))) {
    colnames(Y) <- 1:ncol(Y)
  }
  if (is.null(colnames(X))) {
    colnames(X) <- 1:ncol(X)
  }
  #On lance ça sur tous les fold puis on moyenne le tout
  Freqs <- Reduce("+", lapply(1:10,Par_fold))/10
  Freqs <- cbind(rep(colnames(Y), each = p), rep(colnames(X),
                                                 q), as.data.frame(Freqs))
  names(Freqs) <- c("Names of the Columns of Y", "Levels of the qualitative variable",nb_replis)
  # Freqs <- mclapply(1:10,Par_fold,  mc.cores = nb.cores)
  return(Freqs)
  
  
}

 # T1<- New_stab_sel_anova (group=X2,Y,nb_repli=1000,nb.cores=32)
# 
# save('T1',file='Frequence_doubleloop.RData')


rmvdata2<-function(X, Sigma12, SNR=1, mu = rep(0, ncol(Sigma12)),sparsity) {
  
  ## problem dimension
  n <- nrow(X); p <- ncol(X); q <- ncol(Sigma12)
  
  ## RANDOM SPARSE MATRIX OF COEFFICIENTS
  ## how many guys
  # s  <- round(sparsity*p*q) 
  s <- sparsity*p*q
  ## where are they
  ij <- arrayInd(sample(1:(p*q), size = s), c(p,q))
  ## filling the sparse Matrix object
  beta <- sparseMatrix(i = ij[, 1], j = ij[, 2],
                       x =runif(s,1,2) * sample(c(-1,1),s,rep=T),
                       dims = c(p,q))*SNR
  
  ## RANDOM GAUSIAN PREDICTORS
  
  ## THE STOCHASTIC NOISE
  white.noise <- matrix(rnorm(n*q),n,q)
  noise <- as.matrix(white.noise %*% Sigma12)
  Y  <- outer(rep(1,n),mu) + X %*% beta  + noise
  
  # r2 <- sum(diag(crossprod(X %*% beta)))/sum(diag(crossprod(Y-outer(rep(1,n),mu))))
  return(list(Y=Y, beta=beta, noise =  noise))
}
# library(foreach)
# library(doMC)

# registerDoMC(cores=20)
Simu_nb_repli <-function(nb_simu=100,nbreplis=c(10,100,500,1000,5000),group,q=1000,phi=0.8,SNR=1,theta =0,sparsity=0.01,nb.cores=1) {
  X <- model.matrix(~ group + 0)
  
  # Different scenarios 
  Grid <-expand.grid(q,phi,SNR,theta,sparsity)
  names(Grid)<-c('q','phi','SNR','theta','sparsity')
  
  # Number of replicats
  Grid <-do.call(rbind,lapply(1:nb_simu,function(i){Grid}))
  
  #Selection de variable par cas
  Stab_sels <-function(q,phi,SNR,theta,sparsity){
    
    Sigma12 <- getSigma12(q, phi,theta , unif=F)
    Data <- rmvdata2 (X=X, Sigma12=Sigma12,SNR=SNR,sparsity=sparsity) 
    Y <- Data$Y 
    B<-ifelse(as.numeric(Data$beta)!=0,1,0)
 
      # browser()
      Frequencies=New_stab_sel_anova (group,as.matrix(Y),nb_replis=nbreplis,nb.cores=nb.cores)
      return(cbind.data.frame(Frequencies,q=q,phi=phi,SNR=SNR,theta=theta,sparsity=sparsity,Real=B))
    
  }
  do.call(rbind,mapply(q=Grid[,1],phi=Grid[,2],SNR=Grid[,3],theta=Grid[,4],sparsity=Grid[,5], SIMPLIFY = F, Stab_sels ))
}


Selon_repli<- Simu_nb_repli(nb_simu=1,nbreplis=c(10,100,500,1000,5000),group=X2,q=1000,phi=c(0.7,0.9),SNR=c(1,2,10),theta =0,sparsity=c(0.01,0.3),nb.cores=32)
# Selon_repli<- Simu_nb_repli(nb_simu=1,nbreplis=c(10,20),group=X2,q=100,phi=c(0.7,0.9),SNR=c(1),theta =0,sparsity=c(0.01),nb.cores=1)

save(Selon_repli,file='Selon_nbrep.RData')




