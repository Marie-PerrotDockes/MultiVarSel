---
title: "Introduction to MultiVarSel"
author: "Marie Perrot-Dockès, Céline Lévy-Leduc, Julien Chiquet"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction and installation

This vignette explains basic usage of \textbf{MultiVarSel}, an R package to perform variable selection in the multivariate linear model
taking into account the dependence that may exist between the responses. The model can be described as follows : 
\begin{equation}\label{eq:model}
  Y=XB+E,
\end{equation}
where $Y$ is a $n\times q$ matrix of responses, $X$ is a $n \times p$ matrix of covariables, $B$ is a $p\times q$ sparse matrix of coefficients and $E$ is a random error matrix such that $\forall i \in \{1,\cdots,n\}$, $E_i = (E_{i,1},\dots,E_{i,q})\stackrel{iid}{\sim}\mathcal{N}(0,\Sigma)$. 
The package consists in estimating $\Sigma$ beforehand  and to plug this estimator in a Lasso criterion, in order to obtain a sparse estimator of the coefficient matrix $B$.

The package has to be installed and then loaded as follows : 

```{r, message = FALSE}
library(MultiVarSel)
```


# Numerical experiments

We first show an application of our methodology to a simulated data set where the covariance matrix $\Sigma$ is the covariance matrix of an AR(1) process.

## Data generation

We start by generating a random error matrix $E$ as described in the introduction as follows.

```{r}
n <- 30
q <- 1000
p <- 5
rho <- 0.9
sparsity <- 0.01
E <- t(replicate(n, arima.sim(q, model = list(ar = rho, ma = 0))))

```

We then generate a sparse matrix $B$ of coefficients and a matrix of covariables $X$.
  
```{r}
s  <- round(sparsity * p * q) 
ij <- arrayInd(sample(1:(p * q), size = s), c(p, q))
values <- runif(s, 1, 2) * sample(c(-1, 1), s, replace = T)
B <- sparseMatrix(
  i = ij[, 1], j = ij[, 2],
  x = values,
  dims = c(p, q))
X <- matrix(rnorm(n * p), n, p)
Y <- X %*% B  + E
```

## Whitening test

To apply our methodology we start by estimating the matrix $E$ which is obtained by fitting a linear model with the design matrix $X$ to all the columns of $Y$ as if they were independent: 

```{r}
E_hat <- residuals(lm(as.matrix(Y) ~ X + 0))
```

We then use a Portmanteau test to check if each row of this matrix  $\widehat{E}$ is a white noise or not. 

```{r}
whitening_test(E_hat)
```
The $p$-value is smaller than 0.05 hence we reject the hypothesis that each row of the residuals matrix is a white noise which is an expected result since each row of $E$ is an AR(1) process.

## Whitening

We then try to remove the dependence among the columns of the residuals matrix by estimating the covariance matrix of the rows of $E$. To estimate it we try different structures for this covariance. The simplest assumption is that each row of $E$ follows an AR(1) process, we also propose a modelling where each row is an ARMA(p,q) process and a nonparametric one where $\Sigma$ is only assumed to be Toeplitz. 
To compare these different dependence modellings we perform a Portmanteau test on the "whitened" matrix  $\widehat{E}\widehat{\Sigma}^{-1/2}$, where  $\widehat{\Sigma}^{-1/2}$ is the square root of the inverse of the estimation of $\Sigma$.


With the following code we test the AR(1), ARMA(1,1) and nonparametric dependence structures :

```{r}
whitening_choice(E_hat, c("AR1", "ARMA", "nonparam"), pAR = 1, qMA = 1)
```

We then select the simplest model that allows us to remove the dependence in the data, in that case the AR(1) modelling.
We compute the square root of the inverse of the estimator of the covariance matrix of each
row of the residuals matrix using the AR(1) modelling as follows:

```{r}
square_root_inv_hat_Sigma <- whitening(E_hat, "AR1", pAR = 1, qMA = 0)
```

In order to whiten the data (remove the dependence), we transform the data as follows:
\begin{equation}\label{eq:model_whitened}
\boldsymbol{Y}\widehat{\boldsymbol{\Sigma}}_q^{-1/2}  
    =\boldsymbol{X}\boldsymbol{B}\widehat{\boldsymbol{\Sigma}}_q^{-1/2} + \boldsymbol{E}\widehat{\boldsymbol{\Sigma}}_q^{-1/2}.
\end{equation}

The idea is then to use the Lasso criterion introduced by Tibshirani in 1996, and available in the R package \texttt{glmnet} on these whitened data.
We recall that in the classical linear model
\[
  \mathcal{Y}= \mathcal{X} \mathcal{B} + \mathcal{E},
\] 
where $\mathcal{Y}$, $\mathcal{B}$ and $\mathcal{E}$ are vectors and $\mathcal{X}$ is a matrix, the Lasso estimator of $\mathcal{B}$ is defined by
\begin{equation*}
  \widehat{\mathcal{B}}(\lambda)=\textrm{Argmin}_\mathcal{B}\left\{\|\mathcal{Y}-\mathcal{X}\mathcal{B}\|_2^2+\lambda\|\mathcal{B}\|_1\right\}.
\end{equation*}

In order to be able to use the Lasso criterion we will apply the $vec$ operator to \eqref{eq:model_whitened}
\begin{equation*}
\begin{aligned}
      {\mathcal{Y}}&=
                     vec(\boldsymbol{Y}\widehat{\boldsymbol{\Sigma}}^{-1/2})  
 = vec(\boldsymbol{X}\boldsymbol{B}\widehat{\boldsymbol{\Sigma}}^{-1/2})
                     +vec(\boldsymbol{E}\widehat{\boldsymbol{\Sigma}}^{-1/2})\\
                   &={((\widehat{\boldsymbol{\Sigma}}^{-1/2})'\otimes \boldsymbol{X})}{vec(\boldsymbol{B})}
                     +{vec(\boldsymbol{E}\widehat{\boldsymbol{\Sigma}}^{-1/2})}\\
                   &={\mathcal{X}}{\mathcal{B}}+{\mathcal{E}}.
\end{aligned}    
\end{equation*}

## Variable selection

The Lasso criterion applied to $\mathcal{Y}=vec(\boldsymbol{Y}\widehat{\boldsymbol{\Sigma}}^{-1/2})$ will provide an estimation of the non null positions of $\mathcal{B}=vec(\boldsymbol{B})$ and hence the non null positions  of $B$. In order to avoid false positive positions we add a stability selection step.  These different steps (whitening, vectorization, Lasso, stability selection) are implemented in the function \texttt{variable\_selection} of the R package \texttt{MultiVarSel}.

```{r, eval = FALSE}
Frequencies <- variable_selection(Y = Y, X = X, nb_replis = 1000, typeDep =  "AR1", parallel = FALSE)
```

Parallel computing is also supported by this function. To make it work, users must 
download the package \textsf{doMC} which is not available on Windows platforms (it is on others).

```{r}
require(doMC)
registerDoMC(cores = 4)
Frequencies <- variable_selection(Y = Y, X = X, nb_replis = 1000, typeDep =  "AR1", parallel = TRUE)
```

In the previous command line, \texttt{nb\_replis} corresponds to the number of replications which is used in the stability selection. The following plot displays the frequencies at which each coefficient of $B$ is considered as being non null. 

```{r,  fig.keep = TRUE}
p <- ggplot(data = Frequencies[Frequencies$Frequencies >= 0.95, ],
           aes(x = Names_of_Y, y = Names_of_X, color = Frequencies, fill = Frequencies)) +
           geom_tile(size = 0.75) + scale_color_gradient2(midpoint = 0.95, mid = 'orange')  + 
           scale_fill_gradient2(midpoint = 0.95, mid = 'orange') + theme_bw() + 
           theme(axis.text.x = element_text(angle=90)) +
           ylab('Levels of X') + xlab('Names of Y')
p
```

If we take a threshold of 0.95, meaning that we keep as non null values only the ones that are considered as non null in more than 95% of the times we have a True Positive Rate equal to `r round(sum(Frequencies$Frequencies > 0.95 & as.numeric(B) !=0) / sum(as.numeric(B)!=0))` and a False Positive Rate equal to `r  round(sum(Frequencies$Frequencies > 0.95 & as.numeric(B) ==0) / sum(as.numeric(B)==0))`.

# An exemple in metabolomics 

In this section we study a LC-MS (Liquid
Chromatography-Mass Spectrometry) data set made of African copals
samples. The samples correspond to ethanolic extracts of copals produced
by trees belonging to two genera Copaifera (C) and Trachylobium (T) with
a second level of classification coming from the geographical provenance
of the Copaifera samples (West (W) or East (E) Africa). Since all the
Trachylobium samples come from East Africa, we can use the modeling
proposed in \eqref{eq:model} where $X$ is a one-way ANOVA design matrix with 3 levels.
Our goal is to identify the most important features (the $m/z$ values) characterizing the different
levels. In order to speed up the computation time, we decided to only study the 200 first columns of the responses matrix.

```{r}
data("copals_camera")
Y <- scale(Y[,1:200]) 
```

In order to analyze the whole responses matrix the last command line has to be replaced by

```{r,eval = FALSE}
Y <- scale(Y) 
```



We build the design matrix as follows

```{r}
X <- model.matrix( ~ group + 0)
```

We start by computing the residuals of the one-way ANOVA model.

```{r}
E_hat <- residuals(lm(as.matrix(Y) ~ X + 0))
```

Then we test if the columns of the residuals are independent using the Portmanteau test.

```{r}
whitening_test(E_hat)
```

The $p$-value is smaller than 0.05 and thus the hypothesis that each row of $E$ is a white noise is rejected. 
We try our different covariance modellings for the residuals and see if one manages to remove the dependence among the columns 
of the residuals matrix by using a Portmanteau test. 

```{r}
whitening_choice(E_hat, c("AR1", "nonparam", "ARMA"), pAR = 1, qMA = 1)
```

From this result, we observe that the non parametric modelling is the only one which removes the dependence.

In this application, the design matrix $X$ is the design matrix of a one-way ANOVA.
In that scenario we recommend to use the argument \texttt{group =} "your qualitative variable" in the \texttt{variable\_selection} function. This argument will ensure that in the cross-validation the different fold are homogeneously distributed among the levels of the qualitative variable.

```{r,warning = FALSE}
Frequencies <- 
  variable_selection(
    Y = Y, 
    group = group, 
    nb_replis = 1000, 
    typeDep = 'ARMA', 
    pAR = 1, qMA = 1,
    parallel = TRUE
  )
```

The following plot displays the selected coefficients in $B$ that is the selected features ($m/z$ values) characterizing the different levels when the threshold is equal to 0.999.

```{r,warning = FALSE}
Frequencies$Names_of_Y <- as.numeric(gsub('X','',Frequencies$Names_of_Y))

p<-ggplot(data=Frequencies[Frequencies$Frequencies>=0.999,],
               aes(x = Names_of_Y, y = Names_of_X, color = Names_of_X, fill = Names_of_X)) + 
               geom_point() + theme_bw() + 
               theme(text= element_text(face ="bold", size = 12))
p
```

# Session Info

```{r, echo = FALSE}
 sessionInfo()
```
