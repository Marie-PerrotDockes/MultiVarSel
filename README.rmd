# MultiVarSel

This is an R package to perform variable selection in the multivariate Linear model
taking into account the dependence that may exist between the responses. It consists in estimating beforehand the covariance matrix Î£ of
the responses and to plug this estimator in a Lasso criterion, in order to obtain a sparse
estimator of the coefficient matrix.


## Introduction and Installation

This vignette explains how to use the package \textbf{MultiVarSel} which is dedicated to the variable
selection  in  high-dimensional    linear  models  taking  into  account  the  dependence
that  may  exist  between  the  columns  of  the  observations  matrix. The model can be described as follows : 

$$
Y=XB+E,
$$
where $Y$ is a $n\times q$ matrix of responses, $X$ a $n \times p$ matrix of covariables, $B$ a sparse matrix of coefficients and $E$ a random error matrix such that $\forall i \in (1,\cdots,n) ; E_i = (E_{i,1},\dots,E_{i,q})\sim\mathcal{N}(0,\Sigma_q)$. 
The package consists in estimating $\Sigma_q$ beforehand  and to plug this estimator in a Lasso criterion, in order to obtain a sparse estimator of the coefficient matrix, $B$.

The package has to be installed and then load as follows : 

```{r}
devtools::install_github("Marie-PerrotDockes/MultiVarSel")
library(MultiVarSel)
```


## Numerical experiment 
 We first show an application of our methodology to a simulated data set. 
We start by generate a random error matrix $E$ as describe in the Introduction.

```{r}
n <- 30
q <- 100
p <- 5
rho <- 0.9
sparsity <- 0.01
Generate_sigma_sqrt <- function(q,rho) {
    diag <- sqrt(1-rho^2) * rho^(0:(q-2))
    diags <- lapply(0:(q-1), function(k) {
        return(c(rho^k, rep(diag[k+1],q-k-1)))
    })
    return(bandSparse(q,k=0:(q-1),diag=diags))
}

Sigma_sqrt <-  Generate_sigma_sqrt(q, rho)

white.noise   <- matrix(rnorm(q*n),n,q)
E <- as.matrix(white.noise %*% Sigma_sqrt )
```

We then generate a sparse matrix $B$ of coefficients and a matrix of covariables $X$.
  
```{r}
s  <- round(sparsity*p*q) 
ij <- arrayInd(sample(1:(p*q), size = s), c(p,q))
B <- sparseMatrix(i = ij[, 1], j = ij[, 2],
                   x = runif(s) * sample(c(-1,1),s,rep=T),
                   dims = c(p,q))

X <- matrix(rnorm(n*p),n,p)
   
Y <- X %*% B  + E
```


To apply our methodology we start by estimate the matrix $E$ by calculate the residuals independently on all the columns of $Y$: 

```{r}
residual <- lm(as.matrix(Y) ~ X - 1)$residuals
```

We then test use a Portmanteau test to check if each row of this matrix is a white noise. 


```{r}
whitening_test(residual)
```
The $p-value$ is really small we reject the hypothesis that each row of the residual matrix is a white noise. 

We then try to remoove the dependance among the columns of the residuals matrix by estimate the covariance matrix of the lines of $E$. To estimate it we try different structure for this covariance. The simplest assumption is that each row of $E$ follows an $AR(1)$ process, we also propose a modelisation where each row is an $ARMA(p,q)$ process and a  nonparametric one where $\Sigma$ is assumed to be Toeplitz. 
To compare this different estimation we perform a Portmenteau test on the matrix "whithened" $residuals\hat{\Sigma_q^{-1/2}}$, where  $\hat{\Sigma_q^{-1/2}}$ is the square root of the inverse of the estimation of $\Sigma$.

```{r}
result <- whitening_choice(residual, c("AR1","ARMA","nonparam"), pAR = 1, qMA = 1)
result
```

We then select the easiest model that whitened the data, in that case the $AR(1)$ modelling.
We compute the square root of the inverse of the estimator of the covariance matrix of each
row of the residuals matrix using the $AR(1)$ modelling as follows :

```{r}
square_root_inv_hat_Sigma <- whitening(residual, "ARMA", pAR = 1, qMA = 0)
```

To perform a variable selection we will transform our data to be abble to use the Lasso criterion introduce by Tibshirani in 1996, and available in the glmnet package.

  In a linear model ${\mathcal{Y}}={\mathcal{X}}\mathcal{B}+{\mathcal{E}}$, the Lasso estimator of $\mathcal{B}$ is
    \begin{equation*}
      \widehat{\mathcal{B}}(\lambda)=\textrm{Argmin}_\mathcal{B}\left\{\|\mathcal{Y}-\mathcal{X}\mathcal{B}\|_2^2+\lambda\|\mathcal{B}\|_1\right\},
    \end{equation*}
where $\mathcal{Y}$ and $\mathcal{B}$ are vector, $\mathcal{E}$ is a white noise and $\mathcal{X}$ is a matrix.
 
In order to be abble to use the Lasso criterion we will use the operator vec to 
    $\boldsymbol{Y}\widehat{\boldsymbol{\Sigma}}_q^{-1/2}  
    =\boldsymbol{X}\boldsymbol{B}\widehat{\boldsymbol{\Sigma}}_q^{-1/2} ++ \boldsymbol{E}\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$
    \begin{align*}
      {\mathcal{Y}}&=
                     vec(\boldsymbol{Y}\widehat{\boldsymbol{\Sigma}}_q^{-1/2})  
=vec(\boldsymbol{X}\boldsymbol{B}\widehat{\boldsymbol{\Sigma}}_q^{-1/2})
                     +vec(\boldsymbol{E}\widehat{\boldsymbol{\Sigma}}_q^{-1/2})\\
                   &=\textcolor{blue}{((\widehat{\boldsymbol{\Sigma}}_q^{-1/2})'\otimes \boldsymbol{X})}\textcolor{green}{vec(\boldsymbol{B})}
                     +\textcolor{red}{vec(\boldsymbol{E}\widehat{\boldsymbol{\Sigma}}_q^{-1/2})}\\
                   &=\textcolor{blue}{\mathcal{X}}\textcolor{green}{\mathcal{B}}+\textcolor{red}{\mathcal{E}}.
    \end{align*}    


We are back to $\mathcal{Y}$ and $\mathcal{B}$ are vectors, $\mathcal{E}$ is a white noise and $\mathcal{X}$ is a matrix. So we can apply the lasso criterion to estimate the non null positions of $\mathcal{B}=vec(\boldsymbol{B})$ and deduct from it the non null positions  of $B$. In order to avoid the false positif we add a stability selection step.  This different steps are iplemented in the function variable selection of the MultiVarSel package.

```{r}
 # source('~/Documents/Multivar_selec/Multivar_selec/MultiVarSel/R/variable_selection.R', echo=TRUE)
Frequencies=variable_selection(Y = Y, X = X, nb_repli = 50)
```


```{r}
p <- ggplot(data = Frequencies[Frequencies$Frequencies >= 0.95, ],
           aes(x = Names_of_Y, y = Names_of_X, color = Frequencies, fill = Frequencies)) +
           geom_tile(size = 0.75) + scale_color_gradient2(midpoint = 0.95, mid = 'orange')  +                        scale_fill_gradient2(midpoint = 0.95, mid = 'orange') +
           theme_bw() + ylab('Levels of X') + xlab('Names of Y')
p
```

If we take a threshold at 0.95, meaning that we keep as non null values only the one that are kept in more than 95% of the times we have a True Positif Rate of `r sum(Frequencies$Frequencies > 0.95 & as.numeric(B) !=0) / sum(as.numeric(B)!=0)` and a False Positive Rate `r sum(Frequencies$Frequencies > 0.95 & as.numeric(B) ==0) / sum(as.numeric(B)==0)`.


## An exemple in metabolomic 

In this section we study a LC-MS (Liquid
Chromatography-Mass Spectrometry) data set made of African copals
samples. The samples correspond to ethanolic extracts of copals produced
by trees belonging to two genera Copaifera (C) and Trachylobium (T) with
a second level of classification coming from the geographical provenance
of the Copaifera samples (West (W) or East (E) Africa). Since all the
Trachylobium samples come from East Africa, we can use the modeling
proposed in Equations (1) and (2) with C = 3 conditions: CE, CW and TE
such that n CE = 9, n CW = 8 and n TE = 13. Our goal is to identify the
most important features (the m/z values) for distinguishing the different
conditions.
In order to have a fast and reproducible exemple we focus on this section on the 200 first metaboliotes.

```{r}
data("copals_camera")
Y <- scale(Y %>% as.matrix() %>% as.data.frame() %>% select(1:200))
```

We start by calculate the residuals of the ANOVA models on each of the metabolites independently.


```{r}
residuals=lm(as.matrix(Y) ~ X - 1)$residuals
```

Then we test if the columns of the residuals are independent using the Portmanteau test.

```{r}
whitening_test(residuals)
```


The $p-value$ is really small and the fact that each lines of $E$ is a white noise is rejected. 
We will try our different modelisations of the covariance of the residuals and see if one manage to remoove the dependance among the columns of the residuals matrix using a Portmanteau test. 


```{r}
result=whitening_choice(residuals, c("AR1", "nonparam", "ARMA"), pAR = 1, qMA = 1)
result
```

The $AR(1)$ modelisation does not manage to remoove the dependance among the data but the two others do. We select the $ARMA(1,1)$ which is simplier than the non parametric.

In this application, the design matrix $X$ is the design matrix of a one-way ANOVA.
 In that sceanrio we recommand to the user to use the argument group = your qualitative variable, of the variable_selection function. This argument will ensure that in the cross validation the different fold are homogeneously distributed among the levels of the qualitative variable.


```{r}
Frequencies <- variable_selection(Y = Y, group = X2, nb_repli = 100, typeDep = 'ARMA', pAR = 1, qMA = 1)
```


```{r}
Frequencies$Names_of_Y <- as.numeric(gsub('X','',Frequencies$Names_of_Y))
p <- ggplot(data = Frequencies[Frequencies$Frequencies >= 0.95, ],
           aes(x = Names_of_Y, y = Names_of_X, color = Frequencies, fill = Frequencies)) +
           geom_tile(size = 0.75) + scale_color_gradient2(midpoint = 0.95, mid = 'orange')  +                        scale_fill_gradient2(midpoint = 0.95, mid = 'orange') +
           theme_bw() + ylab('Levels of X') + xlab('m/z')
p
```


Hereafter, we also provide some information about the R session

```{r}
 sessionInfo()
```

